{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of various implementations of group lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyglmnet import GLM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyglmnet.datasets import fetch_group_lasso_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.regression import CDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age1</th>\n",
       "      <th>age2</th>\n",
       "      <th>age3</th>\n",
       "      <th>lwt1</th>\n",
       "      <th>lwt2</th>\n",
       "      <th>lwt3</th>\n",
       "      <th>white</th>\n",
       "      <th>black</th>\n",
       "      <th>smoke</th>\n",
       "      <th>ptl1</th>\n",
       "      <th>ptl2m</th>\n",
       "      <th>ht</th>\n",
       "      <th>ui</th>\n",
       "      <th>ftv1</th>\n",
       "      <th>ftv2</th>\n",
       "      <th>ftv3m</th>\n",
       "      <th>bwt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.890000e+02</td>\n",
       "      <td>1.890000e+02</td>\n",
       "      <td>1.890000e+02</td>\n",
       "      <td>1.890000e+02</td>\n",
       "      <td>1.890000e+02</td>\n",
       "      <td>1.890000e+02</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>189.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.552502e-18</td>\n",
       "      <td>-4.023824e-17</td>\n",
       "      <td>1.497920e-17</td>\n",
       "      <td>4.200050e-17</td>\n",
       "      <td>-3.546546e-17</td>\n",
       "      <td>8.517584e-17</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.137566</td>\n",
       "      <td>0.391534</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.248677</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>2.944587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.293250e-02</td>\n",
       "      <td>7.293250e-02</td>\n",
       "      <td>7.293250e-02</td>\n",
       "      <td>7.293250e-02</td>\n",
       "      <td>7.293250e-02</td>\n",
       "      <td>7.293250e-02</td>\n",
       "      <td>0.501265</td>\n",
       "      <td>0.345359</td>\n",
       "      <td>0.489390</td>\n",
       "      <td>0.333840</td>\n",
       "      <td>0.175789</td>\n",
       "      <td>0.244494</td>\n",
       "      <td>0.356190</td>\n",
       "      <td>0.433394</td>\n",
       "      <td>0.366395</td>\n",
       "      <td>0.244494</td>\n",
       "      <td>0.729214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.271557e-01</td>\n",
       "      <td>-5.721926e-02</td>\n",
       "      <td>-2.080054e-01</td>\n",
       "      <td>-1.188094e-01</td>\n",
       "      <td>-6.997723e-02</td>\n",
       "      <td>-2.561629e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.833434e-02</td>\n",
       "      <td>-4.899875e-02</td>\n",
       "      <td>-3.193605e-02</td>\n",
       "      <td>-4.725877e-02</td>\n",
       "      <td>-5.166402e-02</td>\n",
       "      <td>-3.334841e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.277210e-03</td>\n",
       "      <td>-1.418129e-02</td>\n",
       "      <td>2.257001e-02</td>\n",
       "      <td>-2.102353e-02</td>\n",
       "      <td>-2.118627e-02</td>\n",
       "      <td>2.696639e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.801564e-02</td>\n",
       "      <td>2.846959e-02</td>\n",
       "      <td>4.508877e-02</td>\n",
       "      <td>2.429189e-02</td>\n",
       "      <td>1.959068e-02</td>\n",
       "      <td>4.450609e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.995370e-01</td>\n",
       "      <td>6.599831e-01</td>\n",
       "      <td>6.181971e-01</td>\n",
       "      <td>2.866443e-01</td>\n",
       "      <td>4.270983e-01</td>\n",
       "      <td>4.157472e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age1          age2          age3          lwt1          lwt2  \\\n",
       "count  1.890000e+02  1.890000e+02  1.890000e+02  1.890000e+02  1.890000e+02   \n",
       "mean   4.552502e-18 -4.023824e-17  1.497920e-17  4.200050e-17 -3.546546e-17   \n",
       "std    7.293250e-02  7.293250e-02  7.293250e-02  7.293250e-02  7.293250e-02   \n",
       "min   -1.271557e-01 -5.721926e-02 -2.080054e-01 -1.188094e-01 -6.997723e-02   \n",
       "25%   -5.833434e-02 -4.899875e-02 -3.193605e-02 -4.725877e-02 -5.166402e-02   \n",
       "50%   -3.277210e-03 -1.418129e-02  2.257001e-02 -2.102353e-02 -2.118627e-02   \n",
       "75%    3.801564e-02  2.846959e-02  4.508877e-02  2.429189e-02  1.959068e-02   \n",
       "max    2.995370e-01  6.599831e-01  6.181971e-01  2.866443e-01  4.270983e-01   \n",
       "\n",
       "               lwt3       white       black       smoke        ptl1  \\\n",
       "count  1.890000e+02  189.000000  189.000000  189.000000  189.000000   \n",
       "mean   8.517584e-17    0.507937    0.137566    0.391534    0.126984   \n",
       "std    7.293250e-02    0.501265    0.345359    0.489390    0.333840   \n",
       "min   -2.561629e-01    0.000000    0.000000    0.000000    0.000000   \n",
       "25%   -3.334841e-02    0.000000    0.000000    0.000000    0.000000   \n",
       "50%    2.696639e-02    1.000000    0.000000    0.000000    0.000000   \n",
       "75%    4.450609e-02    1.000000    0.000000    1.000000    0.000000   \n",
       "max    4.157472e-01    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "            ptl2m          ht          ui        ftv1        ftv2       ftv3m  \\\n",
       "count  189.000000  189.000000  189.000000  189.000000  189.000000  189.000000   \n",
       "mean     0.031746    0.063492    0.148148    0.248677    0.158730    0.063492   \n",
       "std      0.175789    0.244494    0.356190    0.433394    0.366395    0.244494   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "              bwt  \n",
       "count  189.000000  \n",
       "mean     2.944587  \n",
       "std      0.729214  \n",
       "min      0.709000  \n",
       "25%      2.414000  \n",
       "50%      2.977000  \n",
       "75%      3.487000  \n",
       "max      4.990000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/adityagc/Desktop/MS-Research/Experiments/LowBirthWeight/large-lbw/Cleaning/cleaned.csv\", delimiter= \"\\t\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age1     float64\n",
       "age2     float64\n",
       "age3     float64\n",
       "lwt1     float64\n",
       "lwt2     float64\n",
       "lwt3     float64\n",
       "white      int64\n",
       "black      int64\n",
       "smoke      int64\n",
       "ptl1       int64\n",
       "ptl2m      int64\n",
       "ht         int64\n",
       "ui         int64\n",
       "ftv1       int64\n",
       "ftv2       int64\n",
       "ftv3m      int64\n",
       "bwt      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 16)\n",
      "(189,)\n"
     ]
    }
   ],
   "source": [
    "X = df[df.columns.difference([\"bwt\"])].values\n",
    "y = df.loc[:, \"bwt\"].values\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest =  train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyGLMnet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'beta0': array([0.93791745]), 'beta': array([[ 0.00076806],\n",
       "         [ 0.00102933],\n",
       "         [ 0.00127924],\n",
       "         [ 0.07785851],\n",
       "         [ 0.06737551],\n",
       "         [ 0.04798786],\n",
       "         [ 0.00056242],\n",
       "         [-0.00147798],\n",
       "         [ 0.00828744],\n",
       "         [-0.00044345],\n",
       "         [ 0.00554922],\n",
       "         [-0.00236206],\n",
       "         [ 0.00174874],\n",
       "         [-0.02608237],\n",
       "         [-0.04772722],\n",
       "         [ 0.1877793 ]])},\n",
       " {'beta0': array([0.99664952]), 'beta': array([[ 0.0033486 ],\n",
       "         [ 0.00679934],\n",
       "         [ 0.00461789],\n",
       "         [ 0.02084261],\n",
       "         [ 0.01952418],\n",
       "         [ 0.01268346],\n",
       "         [-0.0001609 ],\n",
       "         [-0.00313663],\n",
       "         [ 0.01255528],\n",
       "         [-0.00194305],\n",
       "         [ 0.00902564],\n",
       "         [-0.02050182],\n",
       "         [ 0.00630744],\n",
       "         [-0.02758634],\n",
       "         [-0.04840612],\n",
       "         [ 0.12268366]])},\n",
       " {'beta0': array([1.04738937]), 'beta': array([[ 0.00699207],\n",
       "         [ 0.01287136],\n",
       "         [ 0.00808598],\n",
       "         [ 0.00406301],\n",
       "         [ 0.01473335],\n",
       "         [ 0.00455898],\n",
       "         [-0.00267011],\n",
       "         [-0.00530024],\n",
       "         [ 0.01759776],\n",
       "         [-0.00382247],\n",
       "         [ 0.01237398],\n",
       "         [-0.00247574],\n",
       "         [ 0.01014557],\n",
       "         [-0.02065798],\n",
       "         [-0.03639913],\n",
       "         [ 0.06644347]])},\n",
       " {'beta0': array([1.07095808]), 'beta': array([[ 0.01103028],\n",
       "         [ 0.01899436],\n",
       "         [ 0.01158557],\n",
       "         [ 0.00068276],\n",
       "         [ 0.02787742],\n",
       "         [ 0.0078542 ],\n",
       "         [-0.00621603],\n",
       "         [-0.00874102],\n",
       "         [ 0.023159  ],\n",
       "         [-0.0060586 ],\n",
       "         [ 0.01565361],\n",
       "         [-0.0276769 ],\n",
       "         [ 0.01312101],\n",
       "         [-0.00758004],\n",
       "         [-0.01227936],\n",
       "         [ 0.01576018]])},\n",
       " {'beta0': array([1.0814756]), 'beta': array([[ 0.01546464],\n",
       "         [ 0.0251454 ],\n",
       "         [ 0.01489723],\n",
       "         [-0.00723813],\n",
       "         [ 0.03754573],\n",
       "         [ 0.00928701],\n",
       "         [-0.00938657],\n",
       "         [-0.01252358],\n",
       "         [ 0.0290121 ],\n",
       "         [-0.00856022],\n",
       "         [ 0.01895203],\n",
       "         [-0.0068343 ],\n",
       "         [ 0.01508189],\n",
       "         [-0.00190392],\n",
       "         [-0.00284384],\n",
       "         [ 0.00270926]])},\n",
       " {'beta0': array([1.08235312]), 'beta': array([[ 0.0196432 ],\n",
       "         [ 0.03118351],\n",
       "         [ 0.01821162],\n",
       "         [-0.01522434],\n",
       "         [ 0.04211312],\n",
       "         [ 0.00990003],\n",
       "         [-0.01272638],\n",
       "         [-0.01649535],\n",
       "         [ 0.0350455 ],\n",
       "         [-0.0109542 ],\n",
       "         [ 0.02221826],\n",
       "         [-0.03606464],\n",
       "         [ 0.01673966],\n",
       "         [-0.0128285 ],\n",
       "         [-0.01844322],\n",
       "         [ 0.01648064]])},\n",
       " {'beta0': array([1.08502566]), 'beta': array([[ 0.02366812],\n",
       "         [ 0.03708424],\n",
       "         [ 0.02131214],\n",
       "         [-0.02083274],\n",
       "         [ 0.04543124],\n",
       "         [ 0.01000486],\n",
       "         [-0.0153362 ],\n",
       "         [-0.02014814],\n",
       "         [ 0.04086567],\n",
       "         [-0.01311674],\n",
       "         [ 0.02545456],\n",
       "         [-0.05921399],\n",
       "         [ 0.0184166 ],\n",
       "         [-0.01085529],\n",
       "         [-0.0165785 ],\n",
       "         [ 0.0152136 ]])},\n",
       " {'beta0': array([1.08580941]), 'beta': array([[ 0.0274414 ],\n",
       "         [ 0.04281139],\n",
       "         [ 0.02422395],\n",
       "         [-0.02434881],\n",
       "         [ 0.04627477],\n",
       "         [ 0.00910269],\n",
       "         [-0.01763233],\n",
       "         [-0.02377759],\n",
       "         [ 0.04640336],\n",
       "         [-0.01498202],\n",
       "         [ 0.02864503],\n",
       "         [-0.07748337],\n",
       "         [ 0.02019194],\n",
       "         [-0.03808931],\n",
       "         [-0.05885041],\n",
       "         [ 0.04668008]])},\n",
       " {'beta0': array([1.08956355]), 'beta': array([[ 0.03107302],\n",
       "         [ 0.04840268],\n",
       "         [ 0.02700185],\n",
       "         [-0.02624464],\n",
       "         [ 0.04710531],\n",
       "         [ 0.00827264],\n",
       "         [-0.01945125],\n",
       "         [-0.02704116],\n",
       "         [ 0.05171076],\n",
       "         [-0.01664428],\n",
       "         [ 0.03178657],\n",
       "         [-0.09152518],\n",
       "         [ 0.02222572],\n",
       "         [-0.01150707],\n",
       "         [-0.01896024],\n",
       "         [ 0.01705238]])},\n",
       " {'beta0': array([1.08990766]), 'beta': array([[ 0.03483779],\n",
       "         [ 0.05396903],\n",
       "         [ 0.0296777 ],\n",
       "         [-0.02886054],\n",
       "         [ 0.04763956],\n",
       "         [ 0.00697215],\n",
       "         [-0.02108557],\n",
       "         [-0.03015516],\n",
       "         [ 0.05710499],\n",
       "         [-0.0183883 ],\n",
       "         [ 0.03497376],\n",
       "         [-0.10423439],\n",
       "         [ 0.02357154],\n",
       "         [-0.03702825],\n",
       "         [-0.06012671],\n",
       "         [ 0.04633447]])},\n",
       " {'beta0': array([1.0920033]), 'beta': array([[ 0.03996678],\n",
       "         [ 0.06263241],\n",
       "         [ 0.03383555],\n",
       "         [-0.02639349],\n",
       "         [ 0.04290582],\n",
       "         [ 0.00363968],\n",
       "         [-0.02372427],\n",
       "         [-0.03546607],\n",
       "         [ 0.06487696],\n",
       "         [-0.02014007],\n",
       "         [ 0.03998265],\n",
       "         [-0.1157179 ],\n",
       "         [ 0.0279988 ],\n",
       "         [-0.06285599],\n",
       "         [-0.10990857],\n",
       "         [ 0.07608332]])},\n",
       " {'beta0': array([1.10306186]), 'beta': array([[ 0.03955427],\n",
       "         [ 0.05955617],\n",
       "         [ 0.02762136],\n",
       "         [-0.02767604],\n",
       "         [ 0.03856858],\n",
       "         [-0.01076325],\n",
       "         [-0.03551718],\n",
       "         [-0.06796972],\n",
       "         [ 0.00799322],\n",
       "         [-0.04490268],\n",
       "         [ 0.11360065],\n",
       "         [-0.1519272 ],\n",
       "         [ 0.05300572],\n",
       "         [-0.06295276],\n",
       "         [-0.1183143 ],\n",
       "         [ 0.07675972]])},\n",
       " {'beta0': array([1.10487834]), 'beta': array([[ 0.07439208],\n",
       "         [ 0.11386498],\n",
       "         [ 0.05258276],\n",
       "         [-0.02404356],\n",
       "         [ 0.03233844],\n",
       "         [-0.0134268 ],\n",
       "         [-0.03723647],\n",
       "         [-0.0730878 ],\n",
       "         [ 0.0042578 ],\n",
       "         [-0.013018  ],\n",
       "         [ 0.04497232],\n",
       "         [-0.14555242],\n",
       "         [ 0.06523874],\n",
       "         [-0.07349813],\n",
       "         [-0.14446315],\n",
       "         [ 0.08723508]])}]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = [1,1,1,2,2,2,3,3,4,5,5,6,7,8,8,8]\n",
    "lambdas = [0.01, 0.03, 0.035, 0.04, 0.045, 0.05, 0.07, 0.09, 0.11, 0.13, 0.15, 0.17, 0.19]\n",
    "# [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "gl_glm = GLM(tol=1e-2, group=groups, score_metric=\"LookupError\", alpha=1.0, reg_lambda=lambdas)\n",
    "gl_glm.fit(Xtrain, ytrain)\n",
    "gl_glm.fit_\n",
    "# type(lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 38)\n",
      "(13, 38)\n"
     ]
    }
   ],
   "source": [
    "yhat1 = np.array(gl_glm.predict((Xtest)))\n",
    "print(yhat1.shape)\n",
    "ymat = np.array([ytest,] * len(lambdas))\n",
    "print(ymat.shape)\n",
    "MSEs = [sklearn.metrics.mean_squared_error(ymat[i,:], yhat1[i,:]) for i in range(len(lambdas))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(lambdas, MSEs, 'go-')\n",
    "plt.legend(['Group Lasso: test error',], frameon=False, loc='best')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('MSE')\n",
    "plt.ylim([0, 0.8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "- Loss is comparable to R, ALAMO and literature. However, coefficients are not set exactly to zero by the group lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning-Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.11159995 -0.48801748  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 1.25870319  0.63681325  0.0056234   0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.55719053  0.03982987  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 2.18210223  2.11488747  2.05738045  2.03171959  2.01220578  1.99484779\n",
      "   1.97619771  1.96390543  1.94632864  1.93242874]\n",
      " [ 1.00068624  1.02474018  1.03328738  1.02326885  1.0130725   1.00030845\n",
      "   0.98828893  0.97331065  0.96387336  0.9508014 ]\n",
      " [ 1.21865547  1.1615753   1.10960861  1.0764439   1.04939869  1.02075479\n",
      "   0.99228488  0.96593187  0.9457128   0.9215014 ]\n",
      " [ 0.94289019  0.91361831  0.88746278  0.84254397  0.79412571  0.74398167\n",
      "   0.69429493  0.64485527  0.60127425  0.55335987]\n",
      " [ 0.86640216  0.72642444  0.61607586  0.56701247  0.51362006  0.46130825\n",
      "   0.40796822  0.35679535  0.30684041  0.25656332]\n",
      " [-1.48547372 -0.78882399 -0.12825063  0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-1.06230897 -0.13369207  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 1.57565924  1.13236349  0.58191177  0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.41127512  0.36873908  0.33836316  0.32569161  0.29962569  0.27750659\n",
      "   0.25322737  0.2376632   0.2214665   0.20725277]\n",
      " [ 0.73852884  0.61618385  0.49012092  0.37686069  0.26063832  0.14559958\n",
      "   0.02928503  0.          0.          0.        ]\n",
      " [ 0.32050253  0.3494845   0.38368598  0.38934011  0.40324284  0.41261099\n",
      "   0.42467327  0.42794945  0.43504499  0.43734575]\n",
      " [ 0.18773651  0.19280077  0.2072448   0.20310276  0.20313039  0.20212755\n",
      "   0.20193799  0.18943675  0.17376402  0.15832466]\n",
      " [ 2.21634422  2.21295822  2.19967236  2.21019564  2.21902867  2.23271676\n",
      "   2.24392562  2.2586976   2.26409908  2.27548533]]\n"
     ]
    }
   ],
   "source": [
    "lambdas = range(10)\n",
    "weight_matrix = np.empty([Xtrain.shape[1], len(lambdas)])\n",
    "# print(weight_matrix.shape)\n",
    "for i, lambda_i in enumerate(lambdas):\n",
    "    grp_reg = CDRegressor(alpha=lambda_i, loss='squared', penalty='l1/l2')\n",
    "    grp_reg.fit(Xtrain, ytrain)\n",
    "    yhat3 = grp_reg.predict(Xtest)\n",
    "    grp_reg.score(Xtest, ytest)\n",
    "    weight_matrix[:,i] = grp_reg.coef_\n",
    "prinimt(weight_matrix)\n",
    "with open('weights.csv', 'w') as output:\n",
    "    np.savetxt(output, weight_matrix, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "- Sparsity introduced, however, group structure is lost.\n",
    "- No option to specify groups in model API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fabian Pedregosa implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-c7383c174c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.035\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.045\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.07\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.09\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.17\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.19\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mgrp_lls_l2l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlls_l2l1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_ITER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-154-c7383c174c21>\u001b[0m in \u001b[0;36mlls_l2l1\u001b[0;34m(A, b, penalty, groups, max_iter, rtol, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0minitial_guess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;31m# TODO: better initial guess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 nabla = optimize.newton(phi, initial_guess, dphi, tol=1e-3, maxiter=1e4,\n\u001b[0;32m---> 38\u001b[0;31m                         args=(qp ** 2, eigvals, penalty))\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnabla\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meigvects\u001b[0m \u001b[0;34m/\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0meigvals\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnabla\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/optimize/zeros.py\u001b[0m in \u001b[0;36mnewton\u001b[0;34m(func, x0, fprime, args, tol, maxiter, fprime2)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfprime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# Newton-Rapheson method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mfder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg, optimize\n",
    "\n",
    "MAX_ITER = int(100)\n",
    "\n",
    "def lls_l2l1(A, b, penalty, groups, max_iter=MAX_ITER, rtol=1e-6,\n",
    "             verbose=False):\n",
    "    # .. local variables ..\n",
    "    A, b, groups, penalty = map(np.asanyarray, (A, b, groups, penalty))\n",
    "    if len(groups) != A.shape[1]:\n",
    "        raise ValueError(\"Incorrect shape for groups\")\n",
    "    x = np.zeros(A.shape[1], dtype=A.dtype)\n",
    "    penalty = penalty * A.shape[0]\n",
    "\n",
    "    # .. precompute ..\n",
    "    H = np.dot(A.T, A)\n",
    "    group_labels = [groups == i for i in np.unique(groups)]\n",
    "    H_groups = [np.dot(A[:, g].T, A[:, g]) for g in group_labels]\n",
    "    eig = [linalg.eigh(H_groups[i]) for i in range(len(group_labels))]\n",
    "    Ab = np.dot(A.T, b)\n",
    "\n",
    "    for n_iter in range(max_iter):\n",
    "        def phi(nabla_, qp2, eigvals, penalty):\n",
    "            return 1 - np.sum( qp2 / ((nabla_ * eigvals + penalty) ** 2))\n",
    "        def dphi(nabla_, alpha, eigvals, penalty):\n",
    "            # .. first derivative of phi ..\n",
    "            return np.sum((2 * alpha * eigvals) / ((penalty + nabla_ * eigvals) ** 3))\n",
    "        for i, g in enumerate(group_labels):\n",
    "            # .. shrinkage operator ..\n",
    "            eigvals, eigvects = eig[i]\n",
    "            x_i = x.copy()\n",
    "            x_i[g] = 0.\n",
    "            A_residual = np.dot(H[g], x_i) - Ab[g]\n",
    "            qp = np.dot(eigvects.T, A_residual)\n",
    "            if penalty < linalg.norm(A_residual, 2):\n",
    "                initial_guess = 0. # TODO: better initial guess\n",
    "                nabla = optimize.newton(phi, initial_guess, dphi, tol=1e-3, maxiter=1e4,\n",
    "                        args=(qp ** 2, eigvals, penalty))\n",
    "                x[g] = - nabla * np.dot(eigvects /  (eigvals * nabla + penalty), qp)\n",
    "            else:\n",
    "                x[g] = 0.\n",
    "\n",
    "        # .. dual gap ..\n",
    "        if n_iter % 3:\n",
    "            residual = np.dot(A, x) - b\n",
    "            group_norm = penalty * np.sum([linalg.norm(x[g], 2)\n",
    "                         for i, g in enumerate(group_labels)])\n",
    "            norm_Anu = [linalg.norm(np.dot(H[g], x) - Ab[g]) \\\n",
    "                       for g in group_labels]\n",
    "            if np.any(norm_Anu > penalty):\n",
    "                nnu = residual * np.min(penalty / norm_Anu)\n",
    "            else:\n",
    "                nnu = residual\n",
    "            primal_obj =  .5 * np.dot(residual, residual) + group_norm\n",
    "            dual_obj   = -.5 * np.dot(nnu, nnu) - np.dot(nnu, b)\n",
    "            dual_gap = primal_obj - dual_obj\n",
    "            if verbose:\n",
    "                print ('Relative error: %s' % (dual_gap / dual_obj))\n",
    "            if np.abs(dual_gap / dual_obj) < rtol:\n",
    "                break\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def check_kkt(A, b, x, penalty, groups):\n",
    "    \"\"\"Check KKT conditions for the group lasso\n",
    "    Returns True if conditions are satisfied, False otherwise\n",
    "    \"\"\"\n",
    "    group_labels = [groups == i for i in np.unique(groups)]\n",
    "    penalty = penalty * A.shape[0]\n",
    "    z = np.dot(A.T, np.dot(A, x) - b)\n",
    "    safety_net = 1e-1 # sort of tolerance\n",
    "    for g in group_labels:\n",
    "        if linalg.norm(x[g]) == 0:\n",
    "            if not linalg.norm(z[g]) < penalty + safety_net:\n",
    "                return False\n",
    "        else:\n",
    "            w = - penalty * x[g] / linalg.norm(x[g], 2)\n",
    "            if not np.allclose(z[g], w, safety_net):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "groups = [1,1,1,2,2,2,3,3,4,5,5,6,7,8,8,8]\n",
    "lambdas = [0.01, 0.03, 0.035, 0.04, 0.045, 0.05, 0.07, 0.09, 0.11, 0.13, 0.15, 0.17, 0.19]\n",
    "\n",
    "grp_lls_l2l1 = lls_l2l1(Xtrain, ytrain, 0.01, groups, max_iter=MAX_ITER, rtol=1e-6, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

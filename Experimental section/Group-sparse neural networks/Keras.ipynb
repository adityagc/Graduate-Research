{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group lasso for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the group lasso penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Same setup as 'demo_group_lasso_tensorflow', but we implement training and regularization\n",
    "inside the Keras library. See the other demo for details. The most important part\n",
    "of the code is the L21 class (see below), which can be added to any Keras Dense layer.\n",
    "\n",
    "Note that we do not use TensorBoard here, but we simply plot the loss and number of neurons\n",
    "obtained from a callback class using Matplotlib.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import Regularizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class L21(Regularizer):\n",
    "    \"\"\"Regularizer for L21 regularization.\n",
    "    # Arguments\n",
    "        C: Float; L21 regularization factor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C=0.):\n",
    "        self.C = K.cast_to_floatx(C)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        const_coeff = np.sqrt(K.int_shape(x)[1])\n",
    "        return self.C*const_coeff*K.sum(K.sqrt(K.sum(K.square(x), axis=1)))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'C': float(self.l1)}\n",
    "\n",
    "# Utility function to count active neurons in a Keras model with Dense layers\n",
    "def count_neurons(model):\n",
    "    return np.sum([np.sum(np.sum(np.abs(l.get_weights()[0]), axis=1) > 10**-3) \\\n",
    "                          for l in model.layers])\n",
    "\n",
    "# Callback class to save training loss and the number of neurons\n",
    "class TrainHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.neurons = [count_neurons(self.model)]\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.neurons.append(count_neurons(self.model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = np.genfromtxt('../Cleaning/X_train.csv', delimiter=',')\n",
    "X_tst = np.genfromtxt('../Cleaning/X_test.csv', delimiter=',')\n",
    "y_trn = np.genfromtxt('../Cleaning/Y_train.csv', delimiter=',')\n",
    "y_tst = np.genfromtxt('../Cleaning/Y_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the group-lasso penalized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2565e9e90f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the model in Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL21\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL21\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL21\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Reset session\n",
    "tf.reset_default_graph()\n",
    "K.set_session(tf.Session())\n",
    "\n",
    "# Define the model in Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=X_trn.shape[1],  activation='relu', kernel_regularizer=L21(0.001)))\n",
    "model.add(Dense(15, activation='linear', kernel_regularizer=L21(0.001)))\n",
    "model.add(Dense(1, activation='linear', kernel_regularizer=L21(0.001)))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Get callbacks\n",
    "history = TrainHistory()\n",
    "\n",
    "# Train using batch gradient descent\n",
    "model.fit(X_trn, y_trn, epochs=2500, shuffle=False, batch_size=X_trn.shape[0], verbose=0, callbacks=[history])\n",
    "\n",
    "# Evaluate on test data\n",
    "y_tst_hat = model.predict(X_tst, batch_size=X_tst.shape[0])\n",
    "print('Final loss on test set: ', mean_squared_error(y_tst, y_tst_hat))\n",
    "\n",
    "# Plot the training loss during training\n",
    "plt.figure()\n",
    "plt.plot(history.losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot the active neurons during training\n",
    "plt.figure()\n",
    "plt.plot(history.neurons)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Active neurons')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

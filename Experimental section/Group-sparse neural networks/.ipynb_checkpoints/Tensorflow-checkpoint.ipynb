{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "This script shows how to use group LASSO regularization for neural networks\n",
    "in the TensorFlow library. This type of regularization removes entire neurons \n",
    "during training, by pushing rows of the weight matrices to be zero simultaneously.\n",
    "\n",
    "The use of group LASSO inside neural networks is described here:\n",
    "    \n",
    "    Scardapane, S., Comminiello, D., Hussain, A. and Uncini, A., 2017. \n",
    "    Group sparse regularization for deep neural networks. Neurocomputing, 241, pp.81-89.\n",
    "    \n",
    "A preprint version is available on arXiv: https://arxiv.org/abs/1607.00485.\n",
    "\n",
    "The original code for the paper was written for Lasagne and Theano and is available here:\n",
    "    https://bitbucket.org/ispamm/group-lasso-deep-networks\n",
    "\n",
    "Note that you can combine group LASSO and L1/L2 regularization for better effects. \n",
    "The most important part of the code are lines 29-37. The function 'group_regularization' \n",
    "creates the regularization term (from the list of weight matrices), which can\n",
    "subsequently be added to any error loss (see line 109).\n",
    "\n",
    "We use the TensorBoard to plot the loss and the number of neurons removed \n",
    "during training.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def l21_norm(W):\n",
    "    # Computes the L21 norm of a symbolic matrix W\n",
    "    return tf.reduce_sum(tf.norm(W, axis=1))\n",
    "\n",
    "def group_regularization(v):\n",
    "    # Computes a group regularization loss from a list of weight matrices corresponding\n",
    "    # to the different layers (see line 93 for its use).\n",
    "    const_coeff = lambda W: tf.sqrt(tf.cast(W.get_shape().as_list()[1], tf.float32))\n",
    "    return tf.reduce_sum([tf.multiply(const_coeff(W), l21_norm(W)) for W in v if 'bias' not in W.name])\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Reset everything\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # The directory to save TensorBoard summaries\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    logdir = \"summaries/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "    \n",
    "    # We use a simple regression dataset taken from scikit-learn\n",
    "    from sklearn import datasets\n",
    "    data = datasets.load_boston()\n",
    "    \n",
    "    # Preprocess the inputs to be in [-1,1] and split the data in train/test sets\n",
    "    from sklearn import preprocessing, model_selection\n",
    "    X = preprocessing.MinMaxScaler(feature_range=(-1,+1)).fit_transform(data['data'])\n",
    "    y = preprocessing.MinMaxScaler().fit_transform(data['target'].reshape(-1, 1))\n",
    "    X_trn, X_tst, y_trn, y_tst = model_selection.train_test_split(X, y, test_size=0.25)\n",
    "    \n",
    "    # Placeholders for input and output\n",
    "    x = tf.placeholder(tf.float32, shape=[None, X.shape[1]], name='input')\n",
    "    d = tf.placeholder(tf.float32, shape=[None, 1], name='targets')\n",
    "    \n",
    "    # Helper function to generate a layer\n",
    "    def create_layer(in_var, in_size, out_size):\n",
    "        \n",
    "        # Parameters for input-hidden layer\n",
    "        W = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1), name='W')\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[out_size]), name='bias')\n",
    "        \n",
    "        # Output of the hidden layer\n",
    "        return tf.nn.relu(tf.matmul(in_var, W) + b)\n",
    "    \n",
    "    # We define a simple network with two hidden layers\n",
    "    with tf.name_scope('hidden_1'):\n",
    "        h1 = create_layer(x, X.shape[1], 20)\n",
    "    with tf.name_scope('hidden_2'):\n",
    "        h2 = create_layer(h1, 20, 15)\n",
    "    with tf.name_scope('output'):\n",
    "        y = create_layer(h2, 15, 1)\n",
    "        \n",
    "    # Helper function to check how many neurons are left in a layer\n",
    "    count_neurons = lambda W: tf.reduce_sum(tf.cast(tf.greater(tf.reduce_sum(tf.abs(W), reduction_indices=[1]), 10**-3),tf.float32))\n",
    "    \n",
    "    # Get all trainable variables except biases\n",
    "    v = tf.trainable_variables()  \n",
    "    neurons_summary = tf.summary.scalar('neurons', tf.reduce_sum([count_neurons(W) for W in v if 'bias' not in W.name]))\n",
    "    \n",
    "    # Define the error function\n",
    "    with tf.name_scope('squared_loss'):\n",
    "        loss = tf.reduce_mean(tf.squared_difference(d, y))\n",
    "        \n",
    "    # Compute the regularization term\n",
    "    with tf.name_scope('group_regularization'):\n",
    "        reg_loss = 0.001*group_regularization(v)\n",
    "      \n",
    "    # We attach a logger to the error loss and the regularization part\n",
    "    loss_summary = tf.summary.scalar('loss', loss)\n",
    "    reg_loss_summary = tf.summary.scalar('reg_loss', reg_loss)\n",
    "        \n",
    "    # Merge summaries and write them in output\n",
    "    merged = tf.summary.merge([loss_summary, reg_loss_summary, neurons_summary])\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Initialize the summary writer\n",
    "        train_writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "        \n",
    "        with tf.name_scope('train'):\n",
    "            # Training function\n",
    "            train_step = tf.train.AdamOptimizer().minimize(tf.add(loss, reg_loss))\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(1500):\n",
    "            \n",
    "            # Take one training step\n",
    "            summary, _ = sess.run([merged, train_step], feed_dict={x: X_trn, d: y_trn})\n",
    "            train_writer.add_summary(summary, i)\n",
    "        \n",
    "        print('Final loss on test set: ', sess.run([loss], feed_dict={x: X_tst, d: y_tst}))\n",
    "        \n",
    "    train_writer.flush()\n",
    "    train_writer.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
